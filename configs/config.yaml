# Dataset configuration
# To use a different dataset:
# 1. Create a new DataModule in src/data/
# 2. Update these parameters to match your dataset
# 3. Update the data loading in benchmark.py
#
# Example configurations for different datasets:
#
# ImageNet-style dataset:
# dataset:
#   data_dir: "data/imagenet"
#   batch_size: 32
#   num_workers: 4
#   image_size: 224
#   mean: [0.485, 0.456, 0.406]    # ImageNet means
#   std: [0.229, 0.224, 0.225]     # ImageNet stds
#
# CIFAR-style dataset:
# dataset:
#   data_dir: "data/cifar"
#   batch_size: 128
#   num_workers: 4
#   image_size: 224
#   mean: [0.4914, 0.4822, 0.4465]  # CIFAR means
#   std: [0.2023, 0.1994, 0.2010]   # CIFAR stds
#
# Custom medical dataset:
# dataset:
#   data_dir: "data/medical"
#   batch_size: 16
#   num_workers: 4
#   image_size: 224
#   mean: [0.5, 0.5, 0.5]           # Default if unknown
#   std: [0.5, 0.5, 0.5]            # Default if unknown
#   class_weights: [1.0, 1.2, 0.8]  # Optional class weights
dataset:
  data_dir: "data/dermmnist"
  batch_size: 32
  num_workers: 4
  image_size: 224  # Most models expect 224x224 input size

# Model configurations to benchmark
# To use a different model:
# 1. Replace the 'name' with any HuggingFace vision model identifier
# 2. Ensure image_size matches model's requirements
# 3. Adjust learning_rate and weight_decay if needed
models:
  - name: "google/vit-base-patch16-224"
    num_classes: 7                        # Update based on your dataset
    learning_rate: 0.0001
    weight_decay: 0.01

# Training configuration
training:
  max_epochs: 10
  accelerator: "gpu"
  devices: 1
  strategy: "auto"
  precision: 32  # Use 16 for faster training if your GPU supports it

# Logging configuration
logging:
  project_name: "dermmnist-benchmark"
  save_dir: "logs"
  log_every_n_steps: 50
